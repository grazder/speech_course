{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LxiMiEui-kl"
   },
   "source": [
    "# Seminar 4 and Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfPduSa6i-km"
   },
   "source": [
    "In Lecture 5, you got acquainted with another speech recognition model -- RNN-Transducer (RNN-T). This assignment is again a combination of a seminar and a homework. In them, you will first learn how the training of this model works, and also build and train a small version of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8kaGJYEi-kn"
   },
   "source": [
    "## Seminar 4 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fV6n4F0si-kn"
   },
   "source": [
    "In seminar 4 you will implemement forward and backward algorithms for calculating the RNN-T loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FaTtc0Ui-ko"
   },
   "source": [
    "## Homework 3 (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vS1U54O1i-ko"
   },
   "source": [
    "In this homework you will implement a variant of the RNN-T model. For that, you will have to\n",
    "- implement each part of its architecture: Encoder, Predictor, Joiner\n",
    "- implement the greedy decoding algorithm\n",
    "- train your model on a subset of the LibriSpeech corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_VduyNri-ko"
   },
   "source": [
    "## Submitting results\n",
    "This Jupyter notebook contains both a seminar and a homework.\n",
    "\n",
    "For the seminar deadline please submit this Jupiter Notebook `(.ipynb)` with completed cells of the seminar. Save the artifact to a directory named `{your last name}_{your first name}_sem3` and pack them in `.zip` archive.\n",
    "\n",
    "For the homework deadline please submit this Jupiter Notebook `(.ipynb)` with all cells completed.\n",
    "Save the notebook and model weights to a directory named `{your last name}_{your first name}_hw3` and pack them in `.zip` archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AboXphY7i-kp"
   },
   "source": [
    "# Setup - Install package, download files, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tO44Q_hi-kp",
    "outputId": "9bce96ac-2741-4b47-e317-560c680d8e6d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘week_06_files’: File exists\n",
      "--2022-06-13 13:27:08--  https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_05/utils.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6770 (6.6K) [text/plain]\n",
      "Saving to: ‘week_06_files/utils.py’\n",
      "\n",
      "week_06_files/utils 100%[===================>]   6.61K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-06-13 13:27:08 (33.7 MB/s) - ‘week_06_files/utils.py’ saved [6770/6770]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir week_06_files\n",
    "!wget -O week_06_files/utils.py https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_05/utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmrbnsCEi-kq",
    "outputId": "97cc3698-3dcc-433c-8a9f-7e80ceb904bd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-13 13:27:09--  https://drive.google.com/uc?export=download&confirm=t&id=14vgOVBayQGYv9B1P3hYo3JM56rS6ap3U\n",
      "Resolving drive.google.com (drive.google.com)... 173.194.222.194, 2a00:1450:4010:c0b::c2\n",
      "Connecting to drive.google.com (drive.google.com)|173.194.222.194|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-0c-9s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/dmtshdck6a46oklrjv0dbnbpe59utnsd/1655115975000/02999746975866030610/*/14vgOVBayQGYv9B1P3hYo3JM56rS6ap3U?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2022-06-13 13:27:09--  https://doc-0c-9s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/dmtshdck6a46oklrjv0dbnbpe59utnsd/1655115975000/02999746975866030610/*/14vgOVBayQGYv9B1P3hYo3JM56rS6ap3U?e=download\n",
      "Resolving doc-0c-9s-docs.googleusercontent.com (doc-0c-9s-docs.googleusercontent.com)... 142.251.1.132, 2a00:1450:4010:c1e::84\n",
      "Connecting to doc-0c-9s-docs.googleusercontent.com (doc-0c-9s-docs.googleusercontent.com)|142.251.1.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 44610317 (43M) [application/x-zip]\n",
      "Saving to: ‘week_06_files/model_scripted_epoch_5.pt’\n",
      "\n",
      "week_06_files/model 100%[===================>]  42.54M  90.7MB/s    in 0.5s    \n",
      "\n",
      "2022-06-13 13:27:10 (90.7 MB/s) - ‘week_06_files/model_scripted_epoch_5.pt’ saved [44610317/44610317]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: change link to a link from repository\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://drive.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://drive.google.com/uc?export=download&id=14vgOVBayQGYv9B1P3hYo3JM56rS6ap3U' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=14vgOVBayQGYv9B1P3hYo3JM56rS6ap3U\" -O week_06_files/model_scripted_epoch_5.pt && rm -rf /tmp/cookies.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4u_exS1ii-ks"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import wandb\n",
    "import ipywidgets as widgets\n",
    "import itertools\n",
    "from torch import optim\n",
    "from torchaudio.transforms import RNNTLoss\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VceAkujPi-ks"
   },
   "outputs": [],
   "source": [
    "import week_06_files.utils as utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "n14D_Nzei-ks"
   },
   "outputs": [],
   "source": [
    "snapshot_dir = \"rnn_t_snapshots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txn-Rm7Si-kt",
    "outputId": "7302396f-9300-4b71-a108-0680fce90abd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘rnn_t_snapshots’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir rnn_t_snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BudRMBTSi-ku"
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wg8Vm-pCi-ku"
   },
   "source": [
    "# Lecture recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuKI0Cqgi-ku"
   },
   "source": [
    "## Alignment\n",
    "\n",
    "Let $\\mathbf{x} = (x_1, x_2, \\ldots, x_T)$ be a length $T$ input sequence of arbitrary length beloging to the set $X^*$ of all sequences over some input space $X$. Let $\\mathbf{y} = (y_1, \\ldots, y_U)$ be a length $U$ output sequence belonging to the set $Y^*$ of all sequences over some output space $Y$.\n",
    "\n",
    "Define the *extended output space* $\\overline Y$ as $Y \\cup \\emptyset$, where $\\emptyset$ denotes the null output. The intuitive meaning of $\\emptyset$ is 'output nothing'. The sequence $(y_1, \\emptyset, \\emptyset, y_2, \\emptyset, y_3) \\in \\overline Y^*$ is therefore equivalent to $(y_1, y_2, y_3) \\in Y^*$. We refer to the elements $\\mathbf{a} \\in \\overline Y^*$ as *alignments*, since the location of the null symbols determines an alignment between the input and output sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGNZdYoVi-kv"
   },
   "source": [
    "As we saw in CTC, various alignments can be represented in the form of a table called trellis. An example of how an RNN-T trellis may look like:\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1CfXfkePAESz2n20AABVUw9SaZ_xszxwf\">\n",
    "    \n",
    "    \n",
    "Possible alignments in that trellis:\n",
    "    \n",
    "<p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1ipRlSrznwmoD5gCk7k6G06JeUtqPzDQq\">\n",
    "    \n",
    "The final label can be determined by simply removing the blank characher:\n",
    "    \n",
    "$$\n",
    "    C \\emptyset \\emptyset A \\emptyset T \\emptyset \\to CAT\n",
    "$$\n",
    "$$\n",
    "    \\emptyset \\emptyset \\emptyset C A T \\emptyset \\to CAT\n",
    "$$\n",
    "    \n",
    "Given $\\mathbf{x}$, the RNN transducer defines a conditional distribution $P(\\mathbf{a} \\in \\overline Y^* | \\mathbf{x})$. This distribution is then collapsed onto the following distribution over $Y^*$:\n",
    "    \n",
    "$$\n",
    "    P(\\mathbf y \\in Y^* | \\mathbf x) = \\sum_{\\mathbf a \\in \\mathcal{B}^{-1}(\\mathbf y)} P(\\mathbf a | \\mathbf x),\n",
    "$$\n",
    "    \n",
    "where $\\mathcal B: \\overline Y^* \\mapsto Y^*$ is a function that removes the null symbols from the alignments in $Y^*$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwEecXFFi-kv"
   },
   "source": [
    "## Architecture\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1P2aztCi9Z7ookMbHmWBcGtSmG_JHIiMj\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5onsHhvOi-kv"
   },
   "source": [
    "The RNN-T model consists of three neural networks: Encoder, Predictor and Joiner. The Encoder converts the acoustic feature $x_t$ into a high-level representation $f_t$, where $t$ is time index:\n",
    "\n",
    "$$\n",
    "    f_t = \\mathrm{Encoder}(x_t)\n",
    "$$\n",
    "\n",
    "The Predictor works like an RNN language model, which produces a high-level representation $g_u$ by conditioning on the previous non-blank target $y_{u - 1}$ predicted by the RNN-T model, where $u$ is output label index:\n",
    "\n",
    "$$\n",
    "    g_u = \\mathrm{Predictor}(y_{u - 1})\n",
    "$$\n",
    "\n",
    "Note that the input sequence for the predictor **is prepended with the special symbol** $\\langle s \\rangle$ that defines the start of a sentence.\n",
    "\n",
    "The Joiner is a feed forward network that combines the Encoder output $f_t$ and the Predictor output $g_u$ as\n",
    "\n",
    "$$\n",
    "    h_{t, u} = \\mathrm{Joiner}(f_t, g_u) = \\mathrm{FeedForward}(\\mathrm{ReLU}(f_t + g_u))\n",
    "$$\n",
    "\n",
    "The final posterior for each output token $y$ is obtained after applying the softmax operation:\n",
    "\n",
    "$$\n",
    "    P(y | t, u) = \\mathrm{softmax}(h_{t, u})\n",
    "$$\n",
    "    \n",
    "where $P(y | t, u)$ is a distribution of probabilities to emit $y \\in \\overline Y$ at time step $t$ after $u$ previously generated characters, $t \\in [1, T], u \\in [0, U]$.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1tn1wS3fCVFJGwrYumf5Im6gOFZsxRMV-\">\n",
    "\n",
    "We will further need to work with probabilities of individual tokens $y$ for different $t$ and $u$. Instead of writing each time something like $P(y = C | t = 1, u = 0)$, we will, for the sake of simplicity, write it as $P(C | 1, 0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9r3qFqri-kw"
   },
   "source": [
    "## Training: forward-backward algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoQwWrili-kx"
   },
   "source": [
    "The loss function of RNN-T is the negative log posterior of output label sequence $\\mathbf y$ given acoustic feature $\\mathbf x$:\n",
    "\n",
    "$$\n",
    "    \\mathcal L = -\\ln P(\\mathbf y \\in Y^* | \\mathbf x) = -\\ln \\sum_{\\mathbf a \\in \\mathcal{B}^{-1}(\\mathbf y)} P(\\mathbf a | \\mathbf x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-NRW58ki-kx"
   },
   "source": [
    "To determine $P(\\mathbf a | \\mathbf x)$ for an arbitrary alignment $\\mathbf a$, we need to multiply the probabilities $P(y | t, u)$ of each symbol across the path:\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1O-aykP5Wods7ZESCJDBsBw2MeBo5egW4\">\n",
    "\n",
    "$$\n",
    "    \\mathbf a = C \\emptyset \\emptyset A \\emptyset T \\emptyset\n",
    "$$\n",
    "    \n",
    "$$\n",
    "    P(\\mathbf a | \\mathbf x) = P(C | 1, 0) \\cdot P(\\emptyset | 1, 0) \\cdot P(\\emptyset | 2, 1) \\cdot P(A | 3, 1) \\cdot P(\\emptyset | 3, 2) \\cdot P(T | 3, 2) \\cdot P(\\emptyset | 4, 3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLpPKQsii-kx"
   },
   "source": [
    "There are usually too many possible alignments to compute the loss function by just adding them all up directly. We will use dynamic programming to make this computation feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWE1Dj6oi-ky"
   },
   "source": [
    "Define the *forward variable* $\\alpha(t, u)$ as the probability of outputting $\\mathbf y_{[1:u]}$ during $\\mathbf f_{[1:t]}$. The forward variables for all $1 \\le t \\le T$ and $0 \\le u \\le U$ can be calculated recursively using\n",
    "\n",
    "$$\n",
    "    \\alpha(t, u) = \\alpha(t - 1, u) P(\\emptyset | t - 1, u) + \\alpha(t, u - 1) P(y_{u - 1} | t, u - 1)\n",
    "$$\n",
    "\n",
    "with initial condition $\\alpha(1, 0) = 1$. Here $y_{u - 1}$ is the $(u - 1)$-th symbol from the ground truth label $\\mathbf y$.\n",
    "\n",
    "The total output sequene probability is equal to the forward variable at the terminal node:\n",
    "\n",
    "$$\n",
    "    P(\\mathbf y | \\mathbf x) = \\alpha(T, U) P(\\emptyset | T, U)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJUyOq57i-ky"
   },
   "source": [
    "Define the *backward variable* $\\beta(t, u)$ as the probability of outputting $\\mathbf y_{[u + 1: U]}$ during $\\mathbf f_{[t:T]}$. Then\n",
    "\n",
    "$$\n",
    "    \\beta(t, u) = \\beta(t + 1, u) P(\\emptyset | t, u) + \\beta(t, u + 1) P(y_u | t, u)\n",
    "$$\n",
    "\n",
    "with initial condition $\\beta(T, U) = P(\\emptyset | T, U)$. The final value is $\\beta(1, 0)$.\n",
    "\n",
    "From the definition of the forward and backward variables it follows that their product $\\alpha(t, u) \\beta(t, u)$ at any point $(t, u)$ in the output lattice is equal to the probability of emitting the complete output sequence *if $y_u$ is emitted during transcription step $t$*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7eK_-d9i-ky"
   },
   "source": [
    "# Seminar 4: RNN-T Forward-Backward Algorithm (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0WBsAyai-kz"
   },
   "source": [
    "```\n",
    "[ ] (5 points) Implement a Forward Pass\n",
    "[ ] (5 points) Implement a Backward Pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5n6aVggi-kz"
   },
   "source": [
    "Implement forward and backward passes.\n",
    "\n",
    "\n",
    "### Implementation tips\n",
    "\n",
    "- Note that all indices in the arrays you will work with in your code start with zeros. So, the initial condition for forward algorithm will be $\\alpha(0, 0) = 1$ (and $\\log \\alpha(0, 0) = 0$) and the output value for backward algorithm will be $\\beta(0, 0)$. The recurrent formulas stay the same. Also, don't be confused with the terminal node: you don't have to add it to $\\alpha$- and $\\beta$-arrays. The dynamic starts in the upper left corner for forward variables and in the lower right corner for backward variables.\n",
    "- You will need to do everything in log-domain for calculations to be numercally stable. The function [np.logaddexp](https://numpy.org/doc/stable/reference/generated/numpy.logaddexp.html) might help you with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XALt3C3vrl_B"
   },
   "outputs": [],
   "source": [
    "def log_mult(*args) -> float:\n",
    "    exps = np.prod([np.exp(x) for x in args])\n",
    "    if exps == 0:\n",
    "        return NEG_INF\n",
    "    \n",
    "    return np.log(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EcM082LQi-kz"
   },
   "outputs": [],
   "source": [
    "def forward(log_probs: torch.FloatTensor, targets: torch.LongTensor, \n",
    "            blank: int = -1) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "    \"\"\"\n",
    "    :param log_probs: model outputs after applying log_softmax\n",
    "    :param targets: the target sequence of tokens, represented as integer indexes\n",
    "    :param blank: the index of blank symbol\n",
    "    :return: Tuple[ln alpha, -(ln alpha(T, U) + ln P(blank | T, U))]. The latter term is loss value, which is -ln P(y | x)\n",
    "    \"\"\"\n",
    "    max_T, max_U, D = log_probs.shape\n",
    "    \n",
    "    # here the alpha variable contains logarithm of the alpha variable from the formulas above\n",
    "    alpha = np.zeros((max_T, max_U), dtype=np.float32)\n",
    "    \n",
    "    for t in range(1, max_T):\n",
    "        alpha[t, 0] = log_mult(alpha[t - 1, 0], log_probs[t - 1, 0, blank])\n",
    "\n",
    "    for u in range(1, max_U):\n",
    "        alpha[0, u] = log_mult(alpha[0, u - 1], log_probs[0, u - 1, targets[u - 1]])\n",
    "\n",
    "    for t in range(1, max_T):\n",
    "        for u in range(1, max_U):\n",
    "            up = log_mult(alpha[t, u - 1], log_probs[t, u - 1, targets[u - 1]])\n",
    "            left = log_mult(alpha[t - 1, u], log_probs[t - 1, u, blank])\n",
    "\n",
    "            alpha[t, u] = np.logaddexp(up, left)\n",
    "\n",
    "    cost = -log_mult(alpha[-1, -1], log_probs[-1, -1, blank])\n",
    "    return alpha, cost\n",
    "\n",
    "\n",
    "def backward(log_probs: torch.FloatTensor, targets: torch.LongTensor, \n",
    "             blank: int = -1) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "    \"\"\"\n",
    "    :param log_probs: model outputs after applying log_softmax\n",
    "    :param targets: the target sequence of tokens, represented as integer indexes\n",
    "    :param blank: the index of blank symbol\n",
    "    :return: Tuple[ln beta, -ln beta(0, 0)]. The latter term is loss value, which is -ln P(y | x)\n",
    "    \"\"\"\n",
    "    max_T, max_U, D = log_probs.shape\n",
    "    \n",
    "    # here the beta variable contains logarithm of the beta variable from the formulas above\n",
    "    beta = np.zeros((max_T, max_U), dtype=np.float32)\n",
    "    beta[-1, -1] = log_probs[-1, -1, blank]\n",
    "\n",
    "    for t in reversed(range(max_T - 1)):\n",
    "        beta[t, max_U - 1] = log_mult(beta[t + 1, max_U - 1], log_probs[t, max_U - 1, blank])\n",
    "\n",
    "    for u in reversed(range(max_U - 1)):\n",
    "        beta[max_T - 1, u] = log_mult(beta[max_T - 1, u + 1], log_probs[max_T - 1, u, targets[u]])\n",
    "\n",
    "    for t in reversed(range(max_T - 1)):\n",
    "        for u in reversed(range(max_U - 1)):\n",
    "            right = log_mult(beta[t + 1, u], log_probs[t, u, blank])\n",
    "            down = log_mult(beta[t, u + 1], log_probs[t, u, targets[u]])\n",
    "\n",
    "            beta[t, u] = np.logaddexp(right, down)\n",
    "            \n",
    "    cost = -beta[0, 0]\n",
    "    return beta, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "t-kAfhkAi-k0"
   },
   "outputs": [],
   "source": [
    "def run_test(logits: torch.FloatTensor, targets: torch.LongTensor, \n",
    "             ref_costs: torch.FloatTensor, blank: int = -1) -> None:\n",
    "    \"\"\"\n",
    "    :param logits: model outputs\n",
    "    :param targets: the target sequence of tokens, represented as integer indexes\n",
    "    :param ref_costs: the true values of RNN-T costs for test inputs\n",
    "    :param blank: the index of blank symbol\n",
    "    \"\"\"\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    cost = np.zeros(log_probs.shape[0])\n",
    "    \n",
    "    for batch_id in range(log_probs.shape[0]):        \n",
    "        alphas, cost_alpha = forward(log_probs[batch_id], targets[batch_id], blank=blank)\n",
    "        betas, cost_beta = backward(log_probs[batch_id], targets[batch_id], blank=blank)\n",
    "        np.testing.assert_almost_equal(cost_alpha, cost_beta, decimal=2)\n",
    "        cost[batch_id] = cost_alpha\n",
    "    \n",
    "    np.testing.assert_almost_equal(cost, ref_costs, decimal=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NQ49HxWDi-k0"
   },
   "outputs": [],
   "source": [
    "# Tests\n",
    "\n",
    "'''\n",
    "All logits in tests have shapes in the form (B, T, U, D) where\n",
    "\n",
    "B: batch size\n",
    "T: maximum source sequence length in batch\n",
    "U: maximum target sequence length in batch\n",
    "D: feature dimension of each source sequence element\n",
    "'''\n",
    "\n",
    "# test 1\n",
    "logits = torch.FloatTensor([\n",
    "    0.1, 0.6, 0.1, 0.1, 0.1,\n",
    "    0.1, 0.1, 0.6, 0.1, 0.1,\n",
    "    0.1, 0.1, 0.2, 0.8, 0.1,\n",
    "    0.1, 0.6, 0.1, 0.1, 0.1,\n",
    "    0.1, 0.1, 0.2, 0.1, 0.1,\n",
    "    0.7, 0.1, 0.2, 0.1, 0.1,\n",
    "]).reshape(1, 2, 3, 5)\n",
    "\n",
    "targets = torch.LongTensor([[1, 2]])\n",
    "ref_costs = torch.FloatTensor([5.09566688538])\n",
    "\n",
    "run_test(\n",
    "    logits=logits, \n",
    "    targets=targets, \n",
    "    ref_costs=ref_costs, \n",
    "    blank=-1\n",
    ")\n",
    "\n",
    "# test 2\n",
    "logits = torch.FloatTensor([\n",
    "    0.065357, 0.787530, 0.081592, 0.529716, 0.750675, 0.754135, 0.609764, 0.868140,\n",
    "    0.622532, 0.668522, 0.858039, 0.164539, 0.989780, 0.944298, 0.603168, 0.946783,\n",
    "    0.666203, 0.286882, 0.094184, 0.366674, 0.736168, 0.166680, 0.714154, 0.399400,\n",
    "    0.535982, 0.291821, 0.612642, 0.324241, 0.800764, 0.524106, 0.779195, 0.183314,\n",
    "    0.113745, 0.240222, 0.339470, 0.134160, 0.505562, 0.051597, 0.640290, 0.430733,\n",
    "    0.829473, 0.177467, 0.320700, 0.042883, 0.302803, 0.675178, 0.569537, 0.558474,\n",
    "    0.083132, 0.060165, 0.107958, 0.748615, 0.943918, 0.486356, 0.418199, 0.652408,\n",
    "    0.024243, 0.134582, 0.366342, 0.295830, 0.923670, 0.689929, 0.741898, 0.250005,\n",
    "    0.603430, 0.987289, 0.592606, 0.884672, 0.543450, 0.660770, 0.377128, 0.358021,\n",
    "]).reshape(2, 4, 3, 3)\n",
    "\n",
    "targets = torch.LongTensor([[1, 2], [1, 1]])\n",
    "ref_costs = torch.FloatTensor([4.2806528590890736, 3.9384369822503591])\n",
    "\n",
    "run_test(\n",
    "    logits=logits, \n",
    "    targets=targets, \n",
    "    ref_costs=ref_costs, \n",
    "    blank=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-Kcnm7Ki-k1"
   },
   "source": [
    "# Homework 4: Implementing, training and evaluating your RNN-T ASR model (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7_XForhi-k1"
   },
   "source": [
    "```\n",
    "[ ] (18 points) Build the model\n",
    "[ ] (18 points) Implementing a greedy decoder\n",
    "[ ] (4 points) Train the model \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TIy1M2ICi-k1"
   },
   "outputs": [],
   "source": [
    "BLANK_SYMBOL = \"_\"\n",
    "BOS = \"<BOS>\"\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Maps characters to integers and vice versa\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        for i, ch in enumerate([\"'\", \" \"] + list(string.ascii_lowercase) + [BLANK_SYMBOL, BOS]):\n",
    "            self.char_map[ch] = i\n",
    "            self.index_map[i] = ch\n",
    "        \n",
    "    def text_to_indices(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Maps string to a list of integers\n",
    "        \"\"\"\n",
    "        return [self.char_map[ch] for ch in text]\n",
    "\n",
    "    def indices_to_text(self, labels: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Maps integers back to text\n",
    "        \"\"\"\n",
    "        return \"\".join([self.index_map[i] for i in labels])\n",
    "    \n",
    "    def get_symbol_index(self, sym: str) -> int:\n",
    "        \"\"\"\n",
    "        Returns index for the specified symbol\n",
    "        \"\"\"\n",
    "        return self.char_map[sym]\n",
    "    \n",
    "\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_QF8XMBi-k2"
   },
   "source": [
    "### Utils for creating a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "4f707baa4b9f4f48815fb72493427710",
      "ef4c25a371024bdf927253406971e290",
      "4f1fccccaf24405581512550b150460b",
      "2062a2500273409aadf3e5e29586f8e2",
      "1cec14dfdb064a71856a3022b976abac",
      "6654fd79f3e448a38c6373ab8fae7759",
      "0bae0a8ed8094623a29363af74ddf492",
      "cca2a41b33864b81b633446bc3a3bff5",
      "4aaf5d750dc74dbcb8dfcd777cdb0e7e",
      "ba09d93a279645a79ee55689319bd8ad",
      "44f57e25756849b9b97367c3c3c68b55",
      "3a882ea6495041c4887401960763ab2f",
      "bee8fee35ee54b698dca3cdb5e46ecff",
      "88c955ef37df4a99840a12383d6986ff",
      "a76100803e244691b460a2425c31491e",
      "5bc0704da49342ef8937e8fd01666783",
      "42b2226921734369b6a47171fb2beba4",
      "aac04fe7411f4aa68b551d6102cac07a",
      "ef9fd40aa64c42099e5dc03e9e081e25",
      "9d031a60162f42609c2f0ba7633b00f4",
      "8155529772674e07afe2f5ee473f6363",
      "1dbc1e531c264fe8818412ea62fb91d7"
     ]
    },
    "id": "L_Xmk902i-k2",
    "outputId": "23f73303-714c-486f-ed80-ccf000006fd2"
   },
   "outputs": [],
   "source": [
    "# Download LibriSpeech 100hr training and test data\n",
    "\n",
    "if not os.path.isdir(\"./data\"):\n",
    "    os.makedirs(\"./data\")\n",
    "\n",
    "train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"train-clean-100\", download=True)\n",
    "test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"test-clean\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mLWxbcjMi-k2"
   },
   "outputs": [],
   "source": [
    "# For train you can use SpecAugment data aug here.\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=80),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=27),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "\n",
    "test_audio_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cLsCL472i-k2"
   },
   "outputs": [],
   "source": [
    "def data_processing(data: torchaudio.datasets.librispeech.LIBRISPEECH, \n",
    "                    data_type: str = \"train\") -> Tuple[torch.Tensor, torch.IntTensor, torch.IntTensor, torch.IntTensor]:\n",
    "    \"\"\"\n",
    "    :param data: a LIBRISPEECH dataset\n",
    "    :param data_type: \"train\" or \"test\"\n",
    "    :return: tuple of\n",
    "        spectrograms, shape: (B, T, n_mels)\n",
    "        labels, shape: (B, U)\n",
    "        input_lengths -- the length of each spectrogram in the batch, shape: (B,)\n",
    "        label_lengths -- the length of each text label in the batch, shape: (B,)\n",
    "        where\n",
    "        B: batch size\n",
    "        T: maximum source sequence length in batch\n",
    "        U: maximum target sequence length in batch\n",
    "        D: feature dimension of each source sequence element\n",
    "    \"\"\"\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, _, utterance, _, _, _) in data:\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == 'test':\n",
    "            spec = test_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception('data_type should be train or valid')\n",
    "        spectrograms.append(spec)\n",
    "        label = torch.IntTensor(tokenizer.text_to_indices(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0])\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, torch.IntTensor(labels), torch.IntTensor(input_lengths), torch.IntTensor(label_lengths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SbrcCk6i-k3"
   },
   "source": [
    "## Build the model (18 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "y57EYzcgi-k3"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class EncoderRNNT(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_size: int, output_dim: int, n_layers: int, \n",
    "                 dropout: float = 0.2, bidirectional: bool = True):\n",
    "        \"\"\"\n",
    "        An RNN-based model that encodes input audio features into a hidden representation. \n",
    "        The architecture is a stack of LSTM's followed by a fully-connected output layer.\n",
    "\n",
    "        :param input_dim: the number of mel-spectrogram features\n",
    "        :param hidden_size: the number of features in the hidden states in LSTM layers\n",
    "        :param output_dim: the output dimension\n",
    "        :param n_layers: the number of stacked LSTM layers\n",
    "        :param dropout: the dropout probability for LSTM layers\n",
    "        :param bidirectional: If True, each LSTM layer becomes bidirectional\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=n_layers,\n",
    "                           dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "        right_hidden = 2 * hidden_size if bidirectional else hidden_size\n",
    "        self.output_proj = nn.Linear(right_hidden, output_dim)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, input_lengths: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        :param inputs: spectrograms, shape: (B, T, n_mels)\n",
    "        :param input_lengths: the lengths of the spectrograms in the batch, shape: (B,)\n",
    "        :return: outputs of the projection layer and hidden states from LSTMs\n",
    "        \"\"\"\n",
    "        padded_states = pack_padded_sequence(inputs, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        x, hidden = self.lstm(padded_states)\n",
    "        x, _ = pad_packed_sequence(x, batch_first=True)\n",
    "\n",
    "        logits = self.output_proj(x)\n",
    "\n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "tCeaQG-2i-k4"
   },
   "outputs": [],
   "source": [
    "encoder = EncoderRNNT(\n",
    "    input_dim=80,\n",
    "    hidden_size=320,\n",
    "    output_dim=512, \n",
    "    n_layers=4,\n",
    "    dropout=0.2,\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "loader = data.DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: data_processing(x, 'test'))\n",
    "spectrograms, labels, input_lengths, label_lengths = next(iter(loader))\n",
    "logits, hidden_states = encoder.forward(spectrograms, input_lengths)\n",
    "\n",
    "assert spectrograms.shape == torch.Size([2, 835, 80])\n",
    "assert logits.shape == torch.Size([2, 835, 512]) \n",
    "assert len(hidden_states) == 2\n",
    "assert hidden_states[0].shape == torch.Size([8, 2, 320])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bhLTyplNi-k4"
   },
   "outputs": [],
   "source": [
    "class DecoderRNNT(nn.Module):\n",
    "    def __init__(self, hidden_size: int, vocab_size: int, output_dim: int, \n",
    "                 n_layers: int, dropout: float = 0.2):\n",
    "        \"\"\"\n",
    "        A simple RNN-based autoregressive language model that takes as input previously generated text tokens\n",
    "        and outputs a hidden representation of the next token\n",
    "\n",
    "        :param hidden_size: the number of features in the hidden states in LSTM layers\n",
    "        :param vocab_size: the number of text tokens in the dictionary\n",
    "        :param output_dim: the output dimension\n",
    "        :param n_layers: the number of stacked LSTM layers\n",
    "        :param dropout: the dropout probability for LSTM layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, \n",
    "                            num_layers=n_layers, dropout=dropout)\n",
    "\n",
    "        self.output_proj = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, input_lengths: Optional[torch.Tensor] = None, \n",
    "                hidden_states: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        :param inputs: labels, shape: (B, U)\n",
    "        :param input_lengths: the lengths of the text labels in the batch, shape: (B,)\n",
    "        :return: outputs of the projection layer and hidden states from LSTMs\n",
    "        \"\"\"\n",
    "        embed_inputs = self.embedding(inputs)\n",
    "\n",
    "        if input_lengths is not None:\n",
    "            # training phase, the code here is close to `forward` of the Encoder \n",
    "            padded_states = pack_padded_sequence(embed_inputs, input_lengths, \n",
    "                                                 batch_first=True, enforce_sorted=False)\n",
    "            outputs, hidden = self.lstm(padded_states)\n",
    "            outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
    "        else:\n",
    "            outputs, hidden = self.lstm(embed_inputs, hidden_states)\n",
    "\n",
    "        outputs = self.output_proj(outputs)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "QNGRqpMji-k4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "decoder = DecoderRNNT(\n",
    "    hidden_size=512,\n",
    "    vocab_size=len(tokenizer.char_map),\n",
    "    output_dim=512, \n",
    "    n_layers=1, \n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "loader = data.DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: data_processing(x, 'test'))\n",
    "spectrograms, labels, input_lengths, label_lengths = next(iter(loader))\n",
    "logits, hidden_states = decoder.forward(labels, label_lengths)\n",
    "\n",
    "assert labels.shape == torch.Size([2, 158])\n",
    "assert logits.shape == torch.Size([2, 158, 512])\n",
    "assert len(hidden_states) == 2\n",
    "assert hidden_states[0].shape == torch.Size([1, 2, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "IaM_Tkzhi-k5"
   },
   "outputs": [],
   "source": [
    "class Joiner(torch.nn.Module):\n",
    "    def __init__(self, joiner_dim: int, num_outputs: int):\n",
    "        \"\"\"\n",
    "        Adds encoder and decoder outputs, applies ReLU and passes the result \n",
    "        through a fully connected layer to get the output logits\n",
    "\n",
    "        :param joiner_dim: the dimension of the encoder and decoder outputs\n",
    "        :num_outputs: the number of text tokens in the dictionary\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(joiner_dim, num_outputs)\n",
    "\n",
    "    def forward(self, encoder_outputs: torch.Tensor, decoder_outputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param encoder_outputs: the encoder outputs (f_t), shape: (B, T, joiner_dim) or (joiner_dim,)\n",
    "        :param decoder_outputs: the decoder outputs (g_u), shape: (B, U, joiner_dim) or (joiner_dim,)\n",
    "        :return: output logits\n",
    "        \"\"\"\n",
    "        if encoder_outputs.dim() == 3 and decoder_outputs.dim() == 3:    # True for training phase\n",
    "            encoder_outputs = encoder_outputs.unsqueeze(2)\n",
    "            decoder_outputs = decoder_outputs.unsqueeze(1)\n",
    "\n",
    "        # Linear(ReLU(f_t + g_u))\n",
    "        out = F.relu(self.linear(encoder_outputs + decoder_outputs))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ovd_OawAi-k5"
   },
   "outputs": [],
   "source": [
    "class RNNTransducer(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        num_classes: int,\n",
    "        input_dim: int,\n",
    "        num_encoder_layers: int = 4,\n",
    "        num_decoder_layers: int = 1,\n",
    "        encoder_hidden_state_dim: int = 320,\n",
    "        decoder_hidden_state_dim: int = 512,\n",
    "        output_dim: int = 512,\n",
    "        encoder_is_bidirectional: bool = True,\n",
    "        encoder_dropout_p: float = 0.2,\n",
    "        decoder_dropout_p: float = 0.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param num_classes: the number of text tokens in the dictionary\n",
    "        :param input_dim: the number of mel-spectrogram features\n",
    "        :param num_encoder_layers: the number of LSTM layers in the encoder\n",
    "        :param num_decoder_layers: the number of LSTM layers in the decoder\n",
    "        :param encoder_hidden_state_dim: the number of features in the hidden states for the encoder\n",
    "        :param decoder_hidden_state_dim: the number of features in the hidden states for the decoder\n",
    "        :param output_dim: the output dimension\n",
    "        :param encoder_is_bidirectional: whether to use bidirectional LSTM's in the encoder\n",
    "        :param encoder_dropout_p: the dropout probability for the encoder\n",
    "        :param decoder_dropout_p: the dropout probability for the decoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = EncoderRNNT(input_dim=input_dim, hidden_size=encoder_hidden_state_dim,\n",
    "                                   output_dim=output_dim, n_layers=num_encoder_layers, \n",
    "                                   dropout=encoder_dropout_p, bidirectional=encoder_is_bidirectional)\n",
    "\n",
    "        # The decoder takes the input <BOS> + the original sequence. \n",
    "        # You need to shift the current label, and F.pad can help with that.\n",
    "        self.decoder = DecoderRNNT(\n",
    "            hidden_size=decoder_hidden_state_dim,\n",
    "            vocab_size=num_classes,\n",
    "            output_dim=output_dim, \n",
    "            n_layers=num_decoder_layers, \n",
    "            dropout=decoder_dropout_p\n",
    "        )\n",
    "        self.joiner = Joiner(output_dim, num_classes)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, input_lengths: torch.Tensor, \n",
    "                targets: torch.Tensor, target_lengths: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param inputs: spectrograms, shape: (B, T, n_mels)\n",
    "        :param input_lengths: the lengths of the spectrograms in the batch, shape: (B,)\n",
    "        :param targets: labels, shape: (B, U)\n",
    "        :param target_lengths: the lengths of the text labels in the batch, shape: (B,)\n",
    "        :return: the output logits, shape: (B, T, U, n_tokens)\n",
    "        \"\"\"\n",
    "        # <BOS> adding\n",
    "        targets = F.pad(targets, (1, 0, 0, 0), 'constant', tokenizer.char_map['<BOS>'])\n",
    "        new_len = target_lengths + 1\n",
    "\n",
    "        encoder_outputs, _ = self.encoder(inputs, input_lengths)\n",
    "        decoder_outputs, _ = self.decoder(targets, new_len)\n",
    "        joiner_out = self.joiner(encoder_outputs, decoder_outputs)\n",
    "        return joiner_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "PnC5EE33i-k6"
   },
   "outputs": [],
   "source": [
    "transducer = RNNTransducer(\n",
    "    num_classes=len(tokenizer.char_map),\n",
    "    input_dim=80,\n",
    "    num_encoder_layers=4,\n",
    "    num_decoder_layers=1,\n",
    "    encoder_hidden_state_dim=320,\n",
    "    decoder_hidden_state_dim=512,\n",
    "    output_dim=512,\n",
    "    encoder_is_bidirectional=True,\n",
    "    encoder_dropout_p=0.2,\n",
    "    decoder_dropout_p=0.2\n",
    ")\n",
    "\n",
    "loader = data.DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: data_processing(x, 'test'))\n",
    "spectrograms, labels, input_lengths, label_lengths = next(iter(loader))\n",
    "result = transducer.forward(spectrograms, input_lengths, labels, label_lengths)\n",
    "\n",
    "assert spectrograms.shape == torch.Size([2, 835, 80])\n",
    "assert labels.shape == torch.Size([2, 158])\n",
    "assert result.shape == torch.Size([2, 835, 159, 30]), result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBdlLxKni-k6"
   },
   "source": [
    "## Implementing a greedy decoder (18 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPZv4-Bdi-k6"
   },
   "source": [
    "<p style=\"text-align:center;\"><img src=\"http://drive.google.com/uc?export=view&id=1tHsoq0ZH0tHSHYlYlw00y8ksF-wHmrmC\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMhA58FEi-k6"
   },
   "source": [
    "Now we know how to train a Transducer, but how do we infer it? Our task is to generate an output sequence $\\mathbf y$ given an input acoustic sequence $\\mathbf x$.\n",
    "\n",
    "Here we will index the encoder outputs $f_t$ starting from zero, because it is more convenient when describing an algorithm.\n",
    "\n",
    "The greedy decoding procedure is as follows:\n",
    "1. Compute $\\{f_0, \\ldots, f_T\\}$ using $\\mathbf x$.\n",
    "2. Set $t = 0$, $u = 0$, $\\mathbf y = []$, $\\mathrm{iteration} = 0$.\n",
    "3. If $u = 0$, set $g_0 = \\mathrm{Encoder}(\\langle s \\rangle)$. If $u > 0$, compute $g_u$ using the last predicted token $\\mathbf y[-1]$.\n",
    "4. Compute $P(y | t, u)$ using $f_t$ and $g_u$.\n",
    "5. If argmax of $P(y | t, u)$ is a label, set $u = u + 1$ and append the new label to $\\mathbf y$. \n",
    "6. If argmax of $P(y | t, u)$ is $\\emptyset$, set $t = t + 1$.\n",
    "7. If $t = T$ or $\\mathrm{iteration} = \\mathrm{max\\_iterations}$, we are done. Else, set $\\mathrm{iteration} = \\mathrm{iteration + 1}$ and go to step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "o3tjfGsji-k7"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decode(model: RNNTransducer, encoder_output: torch.Tensor, max_steps: int = 2000) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :param model: an RNN-T model in eval mode\n",
    "    :param encoder_output: the output of the encoder part of RNN-T, shape: (T, encoder_output_dim)\n",
    "    :param max_steps: the maximum number of decoding steps\n",
    "    :return: the predicted labels\n",
    "    \"\"\"\n",
    "    pred_tokens, hidden_state = [], None\n",
    "    blank = tokenizer.get_symbol_index(BLANK_SYMBOL)\n",
    "    max_time_steps = encoder_output.size(0)\n",
    "    t = 0\n",
    "    u = 0\n",
    "\n",
    "    decoder_input = encoder_output.new_tensor([[tokenizer.get_symbol_index(BOS)]], dtype=torch.long)\n",
    "    decoder_output, hidden_state = model.decoder(decoder_input, hidden_states=hidden_state)\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        prob_t_u = model.joiner(encoder_output.unsqueeze(0), decoder_output)\n",
    "        argmax_prob = torch.argmax(prob_t_u[0, t, 0, :]).item()\n",
    "\n",
    "        if argmax_prob == blank:\n",
    "            t += 1\n",
    "        else:\n",
    "            u += 1\n",
    "            pred_tokens.append(argmax_prob)\n",
    "            decoder_input = encoder_output.new_tensor([[pred_tokens[-1]]], dtype=torch.long)\n",
    "            decoder_output, hidden_state = model.decoder(decoder_input, hidden_states=hidden_state)\n",
    "\n",
    "        if t == max_time_steps:\n",
    "            break\n",
    "\n",
    "    return torch.LongTensor(pred_tokens)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def recognize(model: RNNTransducer, inputs: torch.Tensor, input_lengths: torch.Tensor) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    :param model: an RNN-T model in eval mode\n",
    "    :param inputs: spectrograms, shape: (B, T, n_mels)\n",
    "    :param input_lengths: the lengths of the spectrograms in the batch, shape: (B,)\n",
    "    :return: a list with the predicted labels\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    encoder_outputs, _ = model.encoder(inputs, input_lengths)\n",
    "\n",
    "    for encoder_output in encoder_outputs:\n",
    "        decoded_seq = greedy_decode(model, encoder_output)\n",
    "        outputs.append(decoded_seq)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def get_transducer_predictions(\n",
    "        transducer: RNNTransducer, inputs: torch.Tensor, input_lengths: torch.Tensor, \n",
    "        targets: torch.Tensor, target_lengths: torch.Tensor\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    :param transducer: an RNN-T model in eval mode\n",
    "    :param inputs: spectrograms, shape: (B, T, n_mels)\n",
    "    :param input_lengths: the lengths of the spectrograms in the batch, shape: (B,)\n",
    "    :param targets: labels, shape: (B, U)\n",
    "    :param target_lengths: the lengths of the text labels in the batch, shape: (B,)\n",
    "    :return: a pd.DataFrame with inference results\n",
    "    \"\"\"\n",
    "    predictions = recognize(transducer, inputs, input_lengths)\n",
    "    result = []\n",
    "    for pred, target, target_len in zip(predictions, targets, target_lengths):\n",
    "        label = target[:target_len]\n",
    "        utterance = tokenizer.indices_to_text(list(map(int, label)))\n",
    "        pred_utterance = tokenizer.indices_to_text(list(map(int, pred)))\n",
    "        result.append({\n",
    "            \"ground_truth\": utterance,\n",
    "            \"prediction\": pred_utterance,\n",
    "            \"cer\": utils.cer(utterance, pred_utterance),\n",
    "            \"wer\": utils.wer(utterance, pred_utterance)\n",
    "        })\n",
    "    return pd.DataFrame.from_records(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "FIJtZpE3i-k7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=RNNTransducer\n",
       "  (encoder): RecursiveScriptModule(\n",
       "    original_name=EncoderRNNT\n",
       "    (lstm): RecursiveScriptModule(original_name=LSTM)\n",
       "    (output_proj): RecursiveScriptModule(original_name=Linear)\n",
       "  )\n",
       "  (decoder): RecursiveScriptModule(\n",
       "    original_name=DecoderRNNT\n",
       "    (embedding): RecursiveScriptModule(original_name=Embedding)\n",
       "    (lstm): RecursiveScriptModule(original_name=LSTM)\n",
       "    (output_proj): RecursiveScriptModule(original_name=Linear)\n",
       "  )\n",
       "  (joiner): RecursiveScriptModule(\n",
       "    original_name=Joiner\n",
       "    (linear): RecursiveScriptModule(original_name=Linear)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.jit.load('week_06_files/model_scripted_epoch_5.pt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "mu-sVO4Hi-k8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>prediction</th>\n",
       "      <th>cer</th>\n",
       "      <th>wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>he hoped there would be stew for dinner turnip...</td>\n",
       "      <td>he hoped there would be stew for dinner turnip...</td>\n",
       "      <td>0.132911</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stuff it into you his belly counselled him</td>\n",
       "      <td>stuffed into you his belly counciled him</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>after early nightfall the yellow lamps would l...</td>\n",
       "      <td>after early night fall the yellow lamps would ...</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello bertie any good in your mind</td>\n",
       "      <td>her about he and he good in your mind</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>number ten fresh nelly is waiting on you good ...</td>\n",
       "      <td>none but den fresh now as waiting on you could...</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ground_truth  \\\n",
       "0  he hoped there would be stew for dinner turnip...   \n",
       "1         stuff it into you his belly counselled him   \n",
       "2  after early nightfall the yellow lamps would l...   \n",
       "3                 hello bertie any good in your mind   \n",
       "4  number ten fresh nelly is waiting on you good ...   \n",
       "\n",
       "                                          prediction       cer       wer  \n",
       "0  he hoped there would be stew for dinner turnip...  0.132911  0.250000  \n",
       "1           stuffed into you his belly counciled him  0.142857  0.375000  \n",
       "2  after early night fall the yellow lamps would ...  0.096154  0.333333  \n",
       "3              her about he and he good in your mind  0.352941  0.714286  \n",
       "4  none but den fresh now as waiting on you could...  0.254237  0.545455  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = data.DataLoader(test_dataset, batch_size=5, shuffle=False, collate_fn=lambda x: data_processing(x, 'test'))\n",
    "spectrograms, labels, input_lengths, label_lengths = next(iter(loader))\n",
    "predictions = get_transducer_predictions(\n",
    "    model, spectrograms, input_lengths,\n",
    "    labels, label_lengths\n",
    ")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "_Udw1t9ii-k8"
   },
   "outputs": [],
   "source": [
    "reference_values = [\n",
    "    {\n",
    "        \"gt\": \"he hoped there would be stew for dinner turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour fattened sauce\",\n",
    "        \"prediction\": \"he hoped there would be stew for dinner turnips and characts and bruised potatoes and fat much and pieces to be lateled out in the thick peppered flowerfacton sauce\"\n",
    "    },\n",
    "    {\n",
    "        \"gt\": \"stuff it into you his belly counselled him\",\n",
    "        \"prediction\": \"stuffed into you his belly counciled him\"\n",
    "    },\n",
    "    {\n",
    "        \"gt\": \"after early nightfall the yellow lamps would light up here and there the squalid quarter of the brothels\",\n",
    "        \"prediction\": \"after early night fall the yellow lamps would lie how peer and there the squalit quarter of the brothels\"\n",
    "    },\n",
    "    {\n",
    "        \"gt\": \"hello bertie any good in your mind\",\n",
    "        \"prediction\": \"her about he and he good in your mind\"\n",
    "    },\n",
    "    {\n",
    "        \"gt\": \"number ten fresh nelly is waiting on you good night husband\",\n",
    "        \"prediction\": \"none but den fresh now as waiting on you could night husband\"\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "g7Pk9s7ei-k8"
   },
   "outputs": [],
   "source": [
    "for index in range(5):\n",
    "    gt = predictions.iloc[index].ground_truth\n",
    "    prediction = predictions.iloc[index].prediction\n",
    "    assert gt == reference_values[index][\"gt\"]\n",
    "    assert prediction == reference_values[index][\"prediction\"], f'{prediction} - {reference_values[index][\"prediction\"]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKCR9BdFi-k9"
   },
   "source": [
    "## Train your model (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBEOtdeZi-k9"
   },
   "source": [
    "Here you can launch training of the model you've just built. To get **2 points**, provide the curves for test loss, CER and WER from Weights & Biases.\n",
    "\n",
    "After training, you will get the test metric values on the hold-out test set. To get the rest **2 points**, you need to pass the following thresholds:\n",
    "\n",
    "- 0.15 test CER\n",
    "- 0.3 test WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6wyqZtQ8i-k9"
   },
   "outputs": [],
   "source": [
    "def train(model: nn.Module, device: str, train_loader: data.DataLoader,\n",
    "          test_sample: List[torch.Tensor], criterion: nn.Module, optimizer: \n",
    "          torch.optim.Optimizer, epoch: int, eval_period: int = 100) -> None:\n",
    "    \"\"\"\n",
    "    :param model: an RNN-T model\n",
    "    :param device: \"gpu\" or \"cpu\"\n",
    "    :param train_loader: training data loader\n",
    "    :param test_sample: a sample from the test set to log preliminary inference metrics\n",
    "    :param criterion: the loss function\n",
    "    :param optimizer: the training optimizer\n",
    "    :param epoch: the current epoch number\n",
    "    :param eval_period: the number of iterations between evaluations\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "\n",
    "    for batch_idx, _data in tqdm(enumerate(train_loader), total=data_len):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model.forward(spectrograms, input_lengths, labels, label_lengths)   # (batch, time, label_length, n_class)\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "\n",
    "        loss = criterion(\n",
    "            output, \n",
    "            labels, \n",
    "            input_lengths.to(device), \n",
    "            label_lengths.to(device)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % eval_period == 0 or batch_idx == data_len:\n",
    "            wandb.log({'loss_train': loss.item()})\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                spectrograms, labels, input_lengths, label_lengths = test_sample\n",
    "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "                predictions = get_transducer_predictions(\n",
    "                    model, spectrograms, input_lengths, \n",
    "                    labels, label_lengths\n",
    "                )\n",
    "                output = model.forward(spectrograms, input_lengths, labels, label_lengths)\n",
    "                val_loss = criterion(\n",
    "                  output, \n",
    "                  labels, \n",
    "                  input_lengths.to(device), \n",
    "                  label_lengths.to(device)\n",
    "                )\n",
    "                wandb.log({'loss_val': val_loss.item()})\n",
    "                clear_output(wait=True)\n",
    "                print('\\nTrain Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\\tVal loss: {:.6f}'.format(\n",
    "                      epoch, batch_idx * len(spectrograms), data_len,\n",
    "                      100. * batch_idx / len(train_loader), loss.item(), val_loss.item()))\n",
    "                print(f\"cer: {predictions.cer.mean()}, wer: {predictions.wer.mean()}\")\n",
    "                display(predictions)\n",
    "                wandb.log({'cer_val': predictions.cer.mean()})\n",
    "                wandb.log({'wer_val': predictions.wer.mean()})\n",
    "                wandb.log({'val_predictions': wandb.Table(dataframe=predictions)})\n",
    "\n",
    "\n",
    "def test(model: nn.Module, device: str, test_loader: data.DataLoader, \n",
    "         criterion: nn.Module, epoch: int, total_steps: int = None, \n",
    "         log_predictions: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    :param model: an RNN-T model\n",
    "    :param device: \"gpu\" or \"cpu\"\n",
    "    :param test_loader: test data loader\n",
    "    :param criterion: the loss function\n",
    "    :param epoch: the current epoch number\n",
    "    :param total_steps: the number of test steps to perform. If None, the whole test set will be used for evaluation\n",
    "    :param log_predictions: if True, the predicted labels will be logged to the W&B dashboard\n",
    "    \"\"\"\n",
    "    print('Beginning eval...')\n",
    "    model.eval()\n",
    "    test_cer, test_wer, test_loss = [], [], []\n",
    "    test_predictions = []\n",
    "    if total_steps is None:\n",
    "        total_steps = len(test_loader)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for i, _data in tqdm_notebook(enumerate(test_loader), total=total_steps):\n",
    "            if i == total_steps:\n",
    "                break\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            output = model.forward(spectrograms, input_lengths, labels, label_lengths)\n",
    "            loss = criterion(\n",
    "              output, \n",
    "              labels, \n",
    "              input_lengths.to(device), \n",
    "              label_lengths.to(device)\n",
    "            )\n",
    "            test_loss.append(loss.item())\n",
    "            \n",
    "            predictions = get_transducer_predictions(\n",
    "                model, spectrograms, input_lengths, \n",
    "                labels, label_lengths\n",
    "            )\n",
    "            test_cer += list(predictions.cer)\n",
    "            test_wer += list(predictions.wer)\n",
    "            if log_predictions:\n",
    "                test_predictions.append(predictions)\n",
    "\n",
    "    avg_cer = np.mean(test_cer)\n",
    "    avg_wer = np.mean(test_wer)\n",
    "    avg_loss = np.mean(test_loss)\n",
    "    \n",
    "    if total_steps < len(test_loader):\n",
    "        wandb.log({\n",
    "            'loss_test': avg_loss, \n",
    "            'avg_cer': avg_cer, \n",
    "            'avg_wer': avg_wer\n",
    "        })\n",
    "    else:\n",
    "        wandb.log({\n",
    "            'loss_test_final': avg_loss, \n",
    "            'avg_cer_final': avg_cer, \n",
    "            'avg_wer_final': avg_wer\n",
    "        })\n",
    "    if log_predictions:\n",
    "        wandb.log({'test_predictions': wandb.Table(dataframe=pd.concat(test_predictions, ignore_index=True))})\n",
    "        \n",
    "    print('Epoch: {:d}, Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(\n",
    "        epoch, avg_loss, avg_cer, avg_wer))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "lOWGMWf9i-k-",
    "outputId": "4bb03d7e-413f-4571-b024-618169146b8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found! 🎉\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(7)\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU found! 🎉')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('Only CPU found! 💻')\n",
    "    device = 'cpu'\n",
    "\n",
    "# Hyperparameters for your model\n",
    "\n",
    "hparams = {\n",
    "    'model': {\n",
    "        'num_classes': len(tokenizer.char_map),\n",
    "        'input_dim': 80,\n",
    "        'num_encoder_layers': 4,\n",
    "        'num_decoder_layers': 1,\n",
    "        'encoder_hidden_state_dim': 320,\n",
    "        'decoder_hidden_state_dim': 512,\n",
    "        'output_dim': 512,\n",
    "        'encoder_is_bidirectional': True,\n",
    "        'encoder_dropout_p': 0.2,\n",
    "        'decoder_dropout_p': 0.2\n",
    "    },\n",
    "    'data': {\n",
    "        'batch_size': 3,\n",
    "        'epochs': 10,\n",
    "        'learning_rate': 1e-4\n",
    "    }\n",
    "}\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=hparams['data']['batch_size'], \n",
    "                               shuffle=True, collate_fn=lambda x: data_processing(x), **kwargs)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=hparams['data']['batch_size'], \n",
    "                              shuffle=False, collate_fn=lambda x: data_processing(x, 'test'), **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "RPzsOxuai-k-",
    "outputId": "1f20306a-d430-4ca8-8a71-97fbb052c5ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNNTransducer(\n",
       "  (encoder): EncoderRNNT(\n",
       "    (lstm): LSTM(80, 320, num_layers=4, dropout=0.2, bidirectional=True)\n",
       "    (output_proj): Linear(in_features=640, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): DecoderRNNT(\n",
       "    (embedding): Embedding(30, 512)\n",
       "    (lstm): LSTM(512, 512, dropout=0.2)\n",
       "    (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (joiner): Joiner(\n",
       "    (linear): Linear(in_features=512, out_features=30, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNTransducer(**hparams['model'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "0Y00qHnwi-k_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgrazder\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/speech_course/week_06/wandb/run-20220613_132724-mg5llv6p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/grazder/speech-transducer/runs/mg5llv6p\" target=\"_blank\">leafy-pyramid-10</a></strong> to <a href=\"https://wandb.ai/grazder/speech-transducer\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/grazder/speech-transducer/runs/mg5llv6p?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f1b989e6160>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"speech-transducer\", \n",
    "           group=\"base-architecture\",\n",
    "           config=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6HlgxA9i-k_",
    "outputId": "f87d07dc-27ce-46fa-84ac-cafe7073b69a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 2 [21900/28539 (77%)]\tTrain Loss: 96.022995\tVal loss: 65.177605\n",
      "cer: 0.4079264617239301, wer: 0.7589285714285714\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>prediction</th>\n",
       "      <th>cer</th>\n",
       "      <th>wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>he hoped there would be stew for dinner turnip...</td>\n",
       "      <td>he hope the wouth stood ordinar ternison care ...</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stuff it into you his belly counselled him</td>\n",
       "      <td>stuffiding to you his belly confelling</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ground_truth  \\\n",
       "0  he hoped there would be stew for dinner turnip...   \n",
       "1         stuff it into you his belly counselled him   \n",
       "\n",
       "                                          prediction       cer       wer  \n",
       "0  he hope the wouth stood ordinar ternison care ...  0.506329  0.892857  \n",
       "1             stuffiding to you his belly confelling  0.309524  0.625000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|███▊      | 10951/28539 [1:04:42<3:27:45,  1.41it/s]\u001b[A\n",
      " 38%|███▊      | 10952/28539 [1:04:43<3:02:28,  1.61it/s]\u001b[A\n",
      " 38%|███▊      | 10953/28539 [1:04:43<2:43:07,  1.80it/s]\u001b[A\n",
      " 38%|███▊      | 10954/28539 [1:04:43<2:27:41,  1.98it/s]\u001b[A\n",
      " 38%|███▊      | 10955/28539 [1:04:44<2:15:10,  2.17it/s]\u001b[A\n",
      " 38%|███▊      | 10956/28539 [1:04:44<2:11:06,  2.24it/s]\u001b[A\n",
      " 38%|███▊      | 10957/28539 [1:04:45<2:05:51,  2.33it/s]\u001b[A\n",
      " 38%|███▊      | 10958/28539 [1:04:45<1:58:47,  2.47it/s]\u001b[A\n",
      " 38%|███▊      | 10959/28539 [1:04:45<1:58:54,  2.46it/s]\u001b[A\n",
      " 38%|███▊      | 10960/28539 [1:04:46<1:55:28,  2.54it/s]\u001b[A\n",
      " 38%|███▊      | 10961/28539 [1:04:46<1:55:02,  2.55it/s]\u001b[A\n",
      " 38%|███▊      | 10962/28539 [1:04:47<1:58:38,  2.47it/s]\u001b[A\n",
      " 38%|███▊      | 10963/28539 [1:04:47<1:53:23,  2.58it/s]\u001b[A\n",
      " 38%|███▊      | 10964/28539 [1:04:47<1:55:38,  2.53it/s]\u001b[A\n",
      " 38%|███▊      | 10965/28539 [1:04:48<1:53:44,  2.58it/s]\u001b[A\n",
      " 38%|███▊      | 10966/28539 [1:04:48<1:49:49,  2.67it/s]\u001b[A\n",
      " 38%|███▊      | 10967/28539 [1:04:48<1:52:59,  2.59it/s]\u001b[A\n",
      " 38%|███▊      | 10968/28539 [1:04:49<1:51:21,  2.63it/s]\u001b[A\n",
      " 38%|███▊      | 10969/28539 [1:04:49<1:52:22,  2.61it/s]\u001b[A\n",
      " 38%|███▊      | 10970/28539 [1:04:50<1:49:36,  2.67it/s]\u001b[A\n",
      " 38%|███▊      | 10971/28539 [1:04:50<1:49:23,  2.68it/s]\u001b[A\n",
      " 38%|███▊      | 10972/28539 [1:04:50<1:40:46,  2.91it/s]\u001b[A\n",
      " 38%|███▊      | 10973/28539 [1:04:51<1:40:18,  2.92it/s]\u001b[A\n",
      " 38%|███▊      | 10974/28539 [1:04:51<1:43:44,  2.82it/s]\u001b[A\n",
      " 38%|███▊      | 10975/28539 [1:04:51<1:47:35,  2.72it/s]\u001b[A\n",
      " 38%|███▊      | 10976/28539 [1:04:52<1:51:27,  2.63it/s]\u001b[A\n",
      " 38%|███▊      | 10977/28539 [1:04:52<1:52:31,  2.60it/s]\u001b[A\n",
      " 38%|███▊      | 10978/28539 [1:04:53<1:50:16,  2.65it/s]\u001b[A\n",
      " 38%|███▊      | 10979/28539 [1:04:53<1:51:31,  2.62it/s]\u001b[A\n",
      " 38%|███▊      | 10980/28539 [1:04:53<1:52:04,  2.61it/s]\u001b[A\n",
      " 38%|███▊      | 10981/28539 [1:04:54<1:53:59,  2.57it/s]\u001b[A\n",
      " 38%|███▊      | 10982/28539 [1:04:54<1:53:59,  2.57it/s]\u001b[A\n",
      " 38%|███▊      | 10983/28539 [1:04:54<1:54:54,  2.55it/s]\u001b[A\n",
      " 38%|███▊      | 10984/28539 [1:04:55<1:54:49,  2.55it/s]\u001b[A\n",
      " 38%|███▊      | 10985/28539 [1:04:55<1:55:05,  2.54it/s]\u001b[A\n",
      " 38%|███▊      | 10986/28539 [1:04:56<1:54:45,  2.55it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=hparams['data']['learning_rate'])\n",
    "criterion = RNNTLoss(blank=tokenizer.get_symbol_index(BLANK_SYMBOL), reduction='mean')\n",
    "test_sample = next(iter(test_loader))\n",
    "\n",
    "for epoch in tqdm_notebook(range(1, hparams['data']['epochs'] + 1)):\n",
    "    train(model, device, train_loader, test_sample, criterion, optimizer, epoch, eval_period=50)\n",
    "    utils.save_checkpoint(model, checkpoint_name=f'model_epoch{epoch}.tar', path=snapshot_dir)\n",
    "    wandb.save(f'model_epoch{epoch}.tar')\n",
    "    test(model, device, test_loader, criterion, epoch, total_steps=20, log_predictions=True)\n",
    "\n",
    "utils.save_checkpoint(model, checkpoint_name=f'model.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Dao_2Mj7i-lA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-8e2f297081b0>:84: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, _data in tqdm_notebook(enumerate(test_loader), total=total_steps):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4401bd5d93645f0aee27368668373d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Test set: Average loss: 25.4773, Average CER: 0.183985 Average WER: 0.3551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, device, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: https://wandb.ai/grazder/speech-transducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "homework3_student.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0bae0a8ed8094623a29363af74ddf492": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1cec14dfdb064a71856a3022b976abac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1dbc1e531c264fe8818412ea62fb91d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2062a2500273409aadf3e5e29586f8e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba09d93a279645a79ee55689319bd8ad",
      "placeholder": "​",
      "style": "IPY_MODEL_44f57e25756849b9b97367c3c3c68b55",
      "value": " 5.95G/5.95G [03:41&lt;00:00, 28.2MB/s]"
     }
    },
    "3a882ea6495041c4887401960763ab2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bee8fee35ee54b698dca3cdb5e46ecff",
       "IPY_MODEL_88c955ef37df4a99840a12383d6986ff",
       "IPY_MODEL_a76100803e244691b460a2425c31491e"
      ],
      "layout": "IPY_MODEL_5bc0704da49342ef8937e8fd01666783"
     }
    },
    "42b2226921734369b6a47171fb2beba4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44f57e25756849b9b97367c3c3c68b55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4aaf5d750dc74dbcb8dfcd777cdb0e7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4f1fccccaf24405581512550b150460b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cca2a41b33864b81b633446bc3a3bff5",
      "max": 6387309499,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4aaf5d750dc74dbcb8dfcd777cdb0e7e",
      "value": 6387309499
     }
    },
    "4f707baa4b9f4f48815fb72493427710": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ef4c25a371024bdf927253406971e290",
       "IPY_MODEL_4f1fccccaf24405581512550b150460b",
       "IPY_MODEL_2062a2500273409aadf3e5e29586f8e2"
      ],
      "layout": "IPY_MODEL_1cec14dfdb064a71856a3022b976abac"
     }
    },
    "5bc0704da49342ef8937e8fd01666783": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6654fd79f3e448a38c6373ab8fae7759": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8155529772674e07afe2f5ee473f6363": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88c955ef37df4a99840a12383d6986ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef9fd40aa64c42099e5dc03e9e081e25",
      "max": 346663984,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9d031a60162f42609c2f0ba7633b00f4",
      "value": 346663984
     }
    },
    "9d031a60162f42609c2f0ba7633b00f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a76100803e244691b460a2425c31491e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8155529772674e07afe2f5ee473f6363",
      "placeholder": "​",
      "style": "IPY_MODEL_1dbc1e531c264fe8818412ea62fb91d7",
      "value": " 331M/331M [00:12&lt;00:00, 29.0MB/s]"
     }
    },
    "aac04fe7411f4aa68b551d6102cac07a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba09d93a279645a79ee55689319bd8ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bee8fee35ee54b698dca3cdb5e46ecff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42b2226921734369b6a47171fb2beba4",
      "placeholder": "​",
      "style": "IPY_MODEL_aac04fe7411f4aa68b551d6102cac07a",
      "value": "100%"
     }
    },
    "cca2a41b33864b81b633446bc3a3bff5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef4c25a371024bdf927253406971e290": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6654fd79f3e448a38c6373ab8fae7759",
      "placeholder": "​",
      "style": "IPY_MODEL_0bae0a8ed8094623a29363af74ddf492",
      "value": "100%"
     }
    },
    "ef9fd40aa64c42099e5dc03e9e081e25": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
